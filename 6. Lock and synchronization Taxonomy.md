# 6. Lock & Synchronization Taxonomy

## 6.1 Full Lock Inventory

| Lock | Component | Type | Scope | Contention Risk | Example |
|---|---|---|---|---|---|
| gRPC Rate Limiter | Ingress concurrency gate | `ReentrantLock` (exclusive) | Per-method singleton | **High** — every inbound request acquires this lock -> Can be optimized | <pre><code>if (rateLimiterLock.tryLock()) {<br>  // process<br>} finally {<br>  rateLimiterLock.unlock();<br>}</code></pre> |
| Saga Context Init Lock | Transaction state machine | `ReentrantLock` (exclusive) | Per-transaction instance | **Low** — one lock per transaction, not shared | <pre><code>initialingLock.lock();<br>try {<br>  // initialize saga steps<br>} finally {<br>  initialingLock.unlock();<br>}</code></pre> |
| Worker Queue Internal Lock | Internal thread pool queue | `ReentrantLock` (JDK built-in) | Per worker pool | **Medium** — every task submission and consumption acquires this | <pre><code>new ArrayBlockingQueue<>(queueSize)<br>// JDK uses ReentrantLock inside</code></pre> |
| Recovery Distributed Lock | Cross-pod recovery coordinator | Distributed lock (Redis Pub/Sub) | Cluster-wide singleton | **Low** — only 1 pod wins per scan interval, non-blocking for others | <pre><code>if (!distributedLockApi.tryAcquire(key)) {<br>  return; // fail-fast others<br>}</code></pre> |
| Idempotency Guard Lock | Revert request deduplication | Redis `SET NX` (atomic) | Per-transaction | **Low** — fire-and-forget, no waiting | <pre><code>client.getScript().evalSha(...<br>  Mode.READ_WRITE, <br>  checkAndMarkKey);</code></pre> |
| DB Connection Pool | Database access layer | Lock-free CAS (`ConcurrentBag`) | Per connection pool | **Low–Medium** — degrades to blocking queue under saturation | <pre><code>config.setMaximumPoolSize(16);<br>new HikariDataSource(config);</code></pre> |
| Timeout Tracking Store | Transaction timeout registry | Redis Sorted Set (cluster-sharded) | Per shard (16 buckets) | **Low** — sharding distributes contention across 16 keys | <pre><code>var bucket = key.hashCode() % 16;<br>sortedSet.add(score, key);</code></pre> |
| Metrics Registry | Observability / monitoring | Segment lock (`ConcurrentHashMap`) | Global registry | **Very Low** — read-heavy, written rarely | <pre><code>monitor.counter(Metrics.RATE_LIMIT)<br>// Micrometer uses ConcurrentHashMap</code></pre> |

```
public static class SimpleConcurrencyLimiter {

    private final int maxConcurrency;
    private final ReentrantLock lock = new ReentrantLock(true); // fair
    private int current = 0;

    public SimpleConcurrencyLimiter(int maxConcurrency) {
        if (maxConcurrency < 1) throw new IllegalArgumentException();
        this.maxConcurrency = maxConcurrency;
    }

    public boolean tryAcquire() {
        lock.lock();
        try {
            if (current <= maxConcurrency) {
                current++;
                return true;
            }
            return false;
        } finally {
            lock.unlock();
        }
    }

    public void release() {
        lock.lock();
        try {
            if (current <= 0) {
                throw new IllegalStateException("Released more times than acquired");
            }
            current--;
        } finally {
            lock.unlock();
        }
    }

    // Bonus: much safer API
    public Permit acquire() throws InterruptedException {
        lock.lockInterruptibly();
        try {
            while (current >= maxConcurrency) {
                // could add wait/notify or use Condition
                // but for simplicity we'll just block here
            }
            current++;
            return new Permit(this);
        } finally {
            lock.unlock();
        }
    }

    public static class Permit implements AutoCloseable {
        private final SimpleConcurrencyLimiter limiter;
        private boolean used = false;

        private Permit(SimpleConcurrencyLimiter limiter) {
            this.limiter = limiter;
        }

        @Override
        public void close() {
            if (!used) {
                used = true;
                limiter.release();
            }
        }
    }
}
```

> **Key observations:**
> - Only **1 lock** sits on the hot path for every single request (the Rate Limiter) — it is the primary optimization target.
> - All other locks are either per-transaction (isolated) or distributed (Redis-level, not JVM-level).
> - The DB connection pool is the only **infrastructure-level** contention point that can silently cascade under sustained load.

## 6.2. The hottest lock in the system
As we saw from the lock's inventory, the grpc Rate limitter is hottest lock in the system - high collion one. What happen is:
At 1,000 TPS, lock acquisitions/sec = 1,000 (acquire) + 1,000 (release in close()) = 2,000 lock ops/sec per method. With two methods (submit + result) = 4,000 lock ops/sec.

ReentrantLock decomposes to:

At 1,000 TPS, lock acquisitions/sec = 1,000 (acquire) + 1,000 (release in `close()`) = **2,000 lock ops/sec** per method. With two methods (submit + result) = 4,000 lock ops/sec.

`ReentrantLock` decomposes to:
1. `AbstractQueuedSynchronizer.tryAcquire()` — fast path: CAS on `state` field (1 machine instruction)
2. If CAS fails (contended): enqueue thread into AQS CLH queue, call `LockSupport.park()` → `futex(FUTEX_WAIT)` → kernel syscall

**When is it contended?** When two gRPC calls arrive simultaneously and both try `lock.lock()`. At 1,000 TPS on a single pod, the mean inter-arrival time is 1ms. The lock hold time is ~100ns (integer add + compare). Probability of collision per arrival = `lock_hold_time / mean_inter_arrival = 100ns / 1ms = 0.01%`. **Contention is negligible in steady state** but **catastrophic under a traffic surge** (e.g., 10,000 TPS burst), where inter-arrival = 100µs and collision probability = 0.1%.

```
Note:
Can consider to replace with CAS ( when contention rate in accepted rate to reduce the park - when the thread is suspended and not run by the OS call LockSupport.park() so here the cost of futex syscall, it is the system call of the linux kernel, designed to implement lock in the efficiently way )
```

Let's give an example of how reentrantLock work under contention. When multiple threads compete for the same lock, the ReentrantLock (powered by the AbstractQueuedSynchronizer - AQS) follows this low-level execution path:

1. Acquisition Attempt & CAS Failure:
Thread A attempts to acquire the lock. The CAS (Compare-And-Swap) operation fails because the synchronization state is already held by Thread B.

2. Enqueuing in AQS:
Thread A encapsulates itself into a "Node" and appends itself to the tail of the AQS wait queue.

3. Thread Suspension (Parking):
Thread A invokes LockSupport.park(). Internally, the JVM executes a futex(FUTEX_WAIT, ...) system call, signaling the OS Kernel to suspend Thread A and move it out of the "Running" state.

4. Lock Release & Wake-up Signal:
Thread B completes its task and releases the lock. It then invokes unpark() on the head of the queue. The JVM executes a futex(FUTEX_ WAKE) system call, prompting the Kernel to wake up Thread A.

5. Resumption & Re-acquisition:
Thread A resumes execution, enters the ready-to-run state, and re-attempts the lock acquisition (typically via another CAS operation) to claim ownership of the synchronization state.

**Lock-free replacement (CAS):**

```java
// Replace ReentrantLock + int with:
private final AtomicInteger current = new AtomicInteger(0);

public boolean acquire() {
    int c;
    do {
        c = current.get();
        if (c >= maxConcurrency) return false;
    } while (!current.compareAndSet(c, c + 1));
    return true;
}

public void release() {
    current.decrementAndGet();
}
```

This is wait-free for the success path and lock-free for the failure path. Eliminates all `futex` syscalls.

## 6.3. `ArrayBlockingQueue` Internal Lock Architecture

`DirectPool` uses `ArrayBlockingQueue`, which internally uses **two** `ReentrantLock` instances:
- `putLock` — acquired by every `pool.execute()` call (producer)
- `takeLock` — acquired by every worker thread polling for work

This is a dual-lock design (Lea, 2004). At high producer throughput:
```
Kafka listener thread → pool.execute() → putLock → enqueue → signal takeLock
DirectPool worker    ← takeLock ← dequeue ← putLock signal
```


