
# 2. JVM Memory Model & Cgroup-Aware Heap Sizing
## 2.1. heap Sizing formula ( run.sh )

```
MAX_RAM=$((4 * 1024 * 1024 * 1024))                        # hard cap: 4 GiB
RAM=$(cat /sys/fs/cgroup/memory.max)                        # cgroupv2 (k8s 1.25+)
# fallback: /sys/fs/cgroup/memory/memory.limit_in_bytes    # cgroupv1
export XMX="$((memory * 75 / 100))m"   # 75% of cgroup limit → Xmx
export XMS=$XMX                         # Xms = Xmx → pre-commit full heap at startup
```

Why we do this? 
JVM always pre-commit full heap to the OS at the startup phase.
We trade of betweeen latency and safety, jVM will not wait OS to provide more memory, the linux system mmap counter full memory which led to overcommit each pod in k8s and each one is always full. This is caused payment system must be always low latency.

## 2.2. Non-heap memory accounting
| Memory Region | Sizing Rule | Estimate (2GiB pod) |
|---|---|---|
| Java Heap (`-Xmx`) | 75% of cgroup RAM | **1536m** |
| Metaspace | Unbounded by default | ~100–150m (Spring + gRPC protos) |
| JIT Code Cache | `-XX:ReservedCodeCacheSize` default = 240m | ~80–120m (active) |
| Thread Stacks | N threads × 512KB | See §1.3 |
| Direct NIO Buffers | Netty off-heap (`-XX:MaxDirectMemorySize`) | ~128–256m |
| GC survivor/metaspace overhead | G1GC internal | ~50m |
| **Total Process RSS** | | **~2.1–2.4 GiB** (2GiB pod is risky)

I recommend adding `-XX:MaxDirectMemorySize=256m` and monitoring `jvm.memory.used{area=nonheap}` which ensure that the raw byte memory is being monitored, ensure no silent OOM the pod in `offguard` memory



## 2.3. Thread stack memory budget

I have a question: Who is responsible for managing the work: the Operating System (Linux) or the Runtime (Java)

### 2.3.1. OS kernel thread 
I still using OS kernel thread, which is not good, industry is moving toward to virtual thread so I encourage to try this if possible. (Os managed by OS - context switch in the OS kernel cost CPU ops, not like the virtual thread managed by the JVM - context switch in the app).

Every JVM thread is a POSIX thread managed by the Linux kernel via clone(CLONE_VM|CLONE_THREAD|...), scheduled by the CFS (Completely Fair Scheduler).
```
JVM Thread  →  pthread  →  kernel task_struct  →  CFS run-queue (per-CPU)
```

**Context-switch cost analysis ( the problem ):**

| Event | Kernel ops | Cost (modern x86-64) |
|---|---|---|
| Voluntary yield (I/O block) | `futex_wait` → `schedule()` | ~1–3 µs |
| Involuntary preempt (timeslice) | `schedule()` + TLB flush | ~3–10 µs |
| Cache cold wakeup (cross-NUMA) | `wake_up_process()` + cache miss | ~10–30 µs |

At 250 threads and 1,000 TPS with ~20ms avg I/O wait: ach thread blocks/resumes ~50×/sec → **12,500 context switches/sec** system-wide



### 2.3.2 Thread system analysis 
Each JVM platform thread on Linux maps to one OS kernel thread (1:1 model, no virtual threads). Default stack size on OpenJDK 17 x86-64 = 512KB 
 

Total threads in steady state at 1,000 TPS:

| Thread Group | Count | Stack (MB) |
|---|---|---|
| Netty boss (gRPC server) | 1 | 0.5 |
| Netty workers (gRPC server) | 2 × N_cores (~8) | 8.0 |
| grpc-server-submit (CachedThreadPool) | up to 32 (rate-gated) | 16.0 |
| grpc-server-result (CachedThreadPool) | up to 128 (rate-gated) | 64.0 |
| grpc-server-default | ~5 | 2.5 |
| SubmitDirectPool (fixed) | K (unknown, see §2.4) | K × 0.5 |
| ResumeDirectPool (fixed) | K (unknown) | K × 0.5 |
| Kafka listener threads | 25 | 12.5 |
| HikariCP housekeeping | 4 (one per pool) | 2.0 |
| Redisson Netty I/O | 16 × 2 clusters = 32 | 16.0 |
| Spring scheduler (recovery scan) | 1 | 0.5 |
| JVM internal (GC, Finalizer, etc.) | ~15 | 7.5 |
| **Total** | **~250+ threads** | **~130+ MB** |

> [!NOTE]
> I am thinking that on a 2GiB pod: `1536m heap + 130m stacks + 150m Metaspace + 120m CodeCache + 200m Netty off-heap ≈ 2136m`. This **exceeds the 2GiB pod limit**. But the real report don't show it exceed, meaning always based on testing to decide it should setting or not, and for safety, you can put it to 3Gib

### 2.3.3. CPU Affinity & NUMA considerations

```
  private final AtomicInteger index = new AtomicInteger(0);
```

This is just a simple Atomic integer make sure it don't change simultaneously by multi thread, but there is problem

Imagine this:
```
Thread A (on CPU 1) updates a counter. That number is stored in CPU 1's L1/L2 Cache (very fast memory inside the chip).

If the OS suddenly moves Thread A to CPU 2, CPU 2 doesn't have that value in its cache. It has to reach across the slow motherboard bus to CPU 1 to "sync" the value. This "Cross-NUMA Cache Invalidation" wastes hundreds of CPU cycles while the processor just waits.
```

So on the multi-node k8s:
+ Netty worker threads may wake on different NUMA nodes than the thread that submitted the gRPC work, causing cross-NUMA cache line invalidation on shared
+ HikariCP connection pool uses ConcurrentBag, which has FIFO borrow semantics — connections borrowed by thread A may be returned and then borrowed by thread B on a different NUMA node, flushing the MySQL network socket receive buffer from L3.


->  use --cpu-manager-policy=static in kubelet + Guaranteed QoS pod. This pins the pod's CPUs to a single NUMA node and eliminates cross-NUMA cache misses on hot data structures.