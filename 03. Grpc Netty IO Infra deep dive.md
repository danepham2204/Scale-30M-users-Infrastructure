# 3. gRPC / Netty I/O Architecture Deep Dive

## 3.1 Netty Event Loop Topology (gRPC Server-side)

gRPC Java on Netty uses event loop(epoll_wait as I mentioned in topology) with two phase: boss (acceptor) and worker(handler)

```
┌─────────────────────────────────────────────────────────┐
│                  NioEventLoopGroup                       │
│  Boss Group (1 thread): accept() new TCP connections     │
│  Worker Group (2×cores threads): read/write NIO selects  │
└─────────────────────────┬───────────────────────────────┘
                          │  Channel registered to worker
                          ▼
              ┌───────────────────────-┐
              │  NioEventLoop (worker) │
              │  - epoll_wait()        │
              │  - decode HTTP/2 frame │
              │  - decompress gRPC msg │
              │  - hand off to executor│  ← ServerCallExecutorSupplier
              └───────────┬───────────-┘
                          │
           ┌──────────────▼──────────────────┐
           │   ServerCallExecutorSupplier     │
           │   routes method naming to corresponding existingpool         │
           └──────────────────────────────────┘
```


The Netty worker thread reads the raw bytes, decodes the HTTP/2 frame, inflates the protobuf, and then immediately hands off to the application executor pool.
 
The Netty thread is never blocked waiting for DB or Redis — it only pays the cost of deserialization (~0.1ms for a typical exchange proto).


```
The risk of using CachedThreadPool in here?
```

The cachedThreadpool is unbound so imagine if 1000 simultaneous gRPC calls	come into the service -> it create 1000 threads which let to OOM. So I have a layer rate limiter at netty, whcih make sure if it exceed limit, it rejected and wait for resubmit. The architecture will be like this

```
gRPC request arrives
       │
       ▼
RateLimiterServerInterceptor.interceptCall()  ← happens on Netty worker thread
       │
       ├── acquire()  → current count >= 32? → REJECT immediately (no thread created)
       │
       └── acquire()  → current count < 32?  → ALLOW
              │
              ▼
       ServerCallExecutorSupplier.getExecutor()
              │
              ▼
       submitPool.execute(task)  ← only NOW does CachedThreadPool create/assign a thread
```

## 3.2. gRPC Netty Client (Outbound) — Channel Pool Mechanics

All outbound stubs using AtomicInteger round-robin over a fixed ManagedChannel instance:

```
private final AtomicInteger index = new AtomicInteger(0);

 private BlockingStub getNext() {
    return stubs[Math.abs(index.getAndIncrement()) % configuration.getPoolSize()];
  }
```

Each `ManagedChannel` maintains:
- HTTP/2 connection (single TCP socket, multiplexed streams)
- Internal `OkHttpChannelTransport` or `NettyClientTransport` with its own Netty event loop
- Up to `MAX_CONCURRENT_CALLS_PER_CONNECTION` (default: 100) concurrent streams

For `pool-size=5` (ZAS), the channel pool supports up to `5 × 100 = 500` concurrent outbound ZAS calls. At 1,000 TPS with ~10ms ZAS latency, steady-state concurrent calls = `1000 × 0.01 = 10` —>  well within the pool capacity.

```
But there is still an issue ? 
```

yes, as you can easily see, we have a crash on our system caused by overflow problem. Using getAndIncrement will wrap to Integer.MIN_VALUE after ~2.1 billion calls -> which led to the modulo % channels.length with a negative dividend returns a negative index in Java, causing ArrayIndexOutOfBoundsException.

So I fix it by using (index.getAndIncrement() & Integer.MAX_VALUE) % channels.length. 



**Yup that solved the result now is always >=0**

